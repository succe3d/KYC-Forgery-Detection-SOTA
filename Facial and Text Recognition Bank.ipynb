{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8490d1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import torch\n",
    "import json\n",
    "import os  \n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import cv2 \n",
    "import insightface\n",
    "from insightface.app import FaceAnalysis\n",
    "from datasets import Dataset, Image as HFImage\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import (\n",
    "    DonutProcessor, \n",
    "    VisionEncoderDecoderModel, \n",
    "    Seq2SeqTrainingArguments, \n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# --- Setup Device ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\" Using device: {device}\")\n",
    "print(f\"Faiss version: {faiss.__version__}\")\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "\n",
    "# --- Initialize SOTA Face Model (ArcFace) ---\n",
    "print(\" Initializing SOTA Face Model (ArcFace)...\")\n",
    "# providers=['CUDAExecutionProvider'] requires ONNX Runtime GPU. \n",
    "# If it fails, it will fall back to CPU.\n",
    "face_app = FaceAnalysis(name='buffalo_l', providers=['CUDAExecutionProvider'])\n",
    "face_app.prepare(ctx_id=0, det_size=(640, 640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae99ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_folder(title=\"Select Folder\"):\n",
    "    \"\"\"Opens a dialog to select a folder.\"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    root.attributes('-topmost', True)\n",
    "    return filedialog.askdirectory(title=title)\n",
    "\n",
    "def select_file(title=\"Select File\", filetypes=[(\"All Files\", \"*.*\")]):\n",
    "    \"\"\"Opens a dialog to select a file.\"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    root.attributes('-topmost', True)\n",
    "    return filedialog.askopenfilename(title=title, filetypes=filetypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976ca845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# DATA LOADING CONFIGURATION\n",
    "# ==========================================\n",
    "print(\" Please select the ROOT folder (containing 'positive', 'meta', 'fraud1...', etc.)\")\n",
    "root_dir = select_folder(\"Select IDNet Root Folder\")\n",
    "\n",
    "if not root_dir:\n",
    "    print(\" Selection cancelled.\")\n",
    "else:\n",
    "    print(f\"\\n Root: {root_dir}\")\n",
    "\n",
    "# Define ALL Use Case Folders\n",
    "target_folders = [\n",
    "    \"positive\",\n",
    "    \"fraud1_copy_and_move\",\n",
    "    \"fraud2_face_morphing\",\n",
    "    \"fraud3_face_replacement\",\n",
    "    \"fraud4_combined\",\n",
    "    \"fraud5_inpaint_and_rewrite\",\n",
    "    \"fraud6_crop_and_replace\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2d9d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Metadata ---\n",
    "meta_base_dir = os.path.join(root_dir, \"meta\")\n",
    "meta_subfolders = [\"positive_meta\", \"detailed_with_fraud_info\"]\n",
    "metadata_map = {}\n",
    "\n",
    "if os.path.exists(meta_base_dir):\n",
    "    print(\" Loading metadata files...\")\n",
    "    for subfolder in meta_subfolders:\n",
    "        full_sub_path = os.path.join(meta_base_dir, subfolder)\n",
    "        if os.path.exists(full_sub_path):\n",
    "            print(f\"   Processing: {subfolder}...\")\n",
    "            for filename in os.listdir(full_sub_path):\n",
    "                if filename.lower().endswith(\".json\") or filename.lower().endswith(\".jsonl\"):\n",
    "                    full_path = os.path.join(full_sub_path, filename)\n",
    "                    try:\n",
    "                        with open(full_path, 'r', encoding='utf-8') as f:\n",
    "                            if filename.endswith('.jsonl'):\n",
    "                                raw_data = [json.loads(line) for line in f]\n",
    "                            else:\n",
    "                                raw_data = json.load(f)\n",
    "                            \n",
    "                            if isinstance(raw_data, list):\n",
    "                                for item in raw_data:\n",
    "                                    key = item.get('file_name') or item.get('image_id') or item.get('id')\n",
    "                                    if key: metadata_map[key] = item\n",
    "                            elif isinstance(raw_data, dict):\n",
    "                                metadata_map.update(raw_data)\n",
    "                    except Exception as e:\n",
    "                        print(f\" Error reading {filename}: {e}\")\n",
    "    print(f\" Loaded combined metadata for {len(metadata_map)} items.\")\n",
    "else:\n",
    "    print(\" 'meta' folder not found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f112b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Images & Match Metadata ---\n",
    "dataset_samples = []\n",
    "valid_extensions = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\"}\n",
    "\n",
    "print(\"  Scanning image folders...\")\n",
    "\n",
    "for folder_name in target_folders:\n",
    "    folder_path = os.path.join(root_dir, folder_name)\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\" Warning: Folder not found: {folder_name} (Skipping)\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"   Scanning: {folder_name}...\")\n",
    "    count = 0\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if os.path.splitext(file)[1].lower() in valid_extensions:\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Match metadata\n",
    "                key = file \n",
    "                meta = metadata_map.get(key)\n",
    "                if not meta: meta = metadata_map.get(os.path.splitext(file)[0])\n",
    "\n",
    "                if meta:\n",
    "                    # Tag the data with the specific Use Case/Fraud Type\n",
    "                    is_fraud = \"positive\" not in folder_name\n",
    "                    meta_copy = meta.copy()\n",
    "                    meta_copy[\"is_fraud\"] = is_fraud\n",
    "                    meta_copy[\"fraud_type\"] = folder_name if is_fraud else \"none\"\n",
    "\n",
    "                    dataset_samples.append({\n",
    "                        \"image\": file_path,\n",
    "                        \"text\": json.dumps(meta_copy) \n",
    "                    })\n",
    "                    count += 1\n",
    "    print(f\"      -> Found {count} matched images.\")\n",
    "\n",
    "if len(dataset_samples) == 0:\n",
    "    print(\" No matching images found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd28b327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HuggingFace Dataset\n",
    "if len(dataset_samples) > 0:\n",
    "    hf_dataset = Dataset.from_list(dataset_samples)\n",
    "    hf_dataset = hf_dataset.cast_column(\"image\", HFImage())\n",
    "    print(\" Dataset object created successfully.\")\n",
    "else:\n",
    "    print(\" Dataset creation failed (0 samples).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ce2323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# MODEL TRAINING SETUP\n",
    "# ==========================================\n",
    "model_save_path = \"custom_trained_donut_model\"\n",
    "max_length = 512\n",
    "image_size = [1280, 960]\n",
    "\n",
    "# Initialize Processor\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "processor.tokenizer.chat_template = None \n",
    "processor.feature_extractor.size = image_size[::-1]\n",
    "processor.feature_extractor.do_align_long_axis = False\n",
    "\n",
    "new_special_tokens = [\"<s_name>\", \"</s_name>\", \"<s_id>\", \"</s_id>\", \"<s_address>\", \"</s_address>\"] \n",
    "processor.tokenizer.add_special_tokens({\"additional_special_tokens\": new_special_tokens + [\"<s>\", \"</s>\"]})\n",
    "\n",
    "# Generator Function\n",
    "def transform_generator(sample):\n",
    "    pixel_values = processor(sample[\"image\"].convert(\"RGB\"), random_padding=True, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "    \n",
    "    #FIX: text is already a string now, no need to json.dumps() again\n",
    "    text_str = sample[\"text\"] \n",
    "    target_sequence = \"<s>\" + text_str + \"</s>\"\n",
    "    \n",
    "    input_ids = processor.tokenizer(\n",
    "        target_sequence, add_special_tokens=False, max_length=max_length,\n",
    "        padding=\"max_length\", truncation=True, return_tensors=\"pt\",\n",
    "    )[\"input_ids\"].squeeze(0)\n",
    "    \n",
    "    labels = input_ids.clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# Prepare Iterable Dataset\n",
    "iterable_dataset = hf_dataset.to_iterable_dataset()\n",
    "processed_iterable_dataset = iterable_dataset.map(transform_generator)\n",
    "shuffled_dataset = processed_iterable_dataset.shuffle(seed=42, buffer_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ea38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Initialize Model\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "    model.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids(['<s>'])[0]\n",
    "    model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "    model.decoder.resize_token_embeddings(len(processor.tokenizer))\n",
    "    model.config.encoder.image_size = image_size[::-1]\n",
    "    model.config.decoder.max_length = max_length\n",
    "    \n",
    "    # Training Arguments\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"training_output\",\n",
    "        max_steps=100, # Set low for testing, increase for real training\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        logging_steps=10,\n",
    "        save_steps=50,\n",
    "        remove_unused_columns=True,\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model, \n",
    "        args=training_args, \n",
    "        train_dataset=shuffled_dataset, \n",
    "    )\n",
    "\n",
    "    print(\" Starting Training...\")\n",
    "    trainer.train()\n",
    "    trainer.save_model(model_save_path)\n",
    "    processor.save_pretrained(model_save_path)\n",
    "    print(f\" Model saved to {model_save_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Training failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c781112",
   "metadata": {},
   "source": [
    "#### Load Saved Custom Trained Donut Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fcec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "def load_custom_donut(model_path=\"custom_trained_donut_model\"):\n",
    "    \"\"\"\n",
    "    Loads the fine-tuned Donut model and processor from disk.\n",
    "    Returns: (model, processor) or (None, None) if failed.\n",
    "    \"\"\"\n",
    "    print(f\" Loading Donut model from folder: '{model_path}'...\")\n",
    "    \n",
    "    # 1. Check if the folder actually exists\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\" Error: The folder '{model_path}' does not exist.\")\n",
    "        print(\"    You must run the 'Training' block at least once to create it.\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        # 2. Determine Device (GPU is much faster)\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # 3. Load the Model Weights (pytorch_model.bin)\n",
    "        # This reconstructs the exact neural network you saved.\n",
    "        model = VisionEncoderDecoderModel.from_pretrained(model_path)\n",
    "        model.to(device) # Move to GPU\n",
    "        model.eval()     # Switch to Inference Mode (Critical for accuracy)\n",
    "        \n",
    "        # 4. Load the Processor (Tokenizer + Image config)\n",
    "        # This ensures we process images exactly how the model expects them.\n",
    "        processor = DonutProcessor.from_pretrained(model_path)\n",
    "        \n",
    "        print(f\" Custom Donut Model loaded successfully on {device}!\")\n",
    "        return model, processor\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Critical Error loading model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# --- HOW TO USE IT ---\n",
    "# Instead of training every time, just run this line:\n",
    "loaded_model, loaded_processor = load_custom_donut()\n",
    "\n",
    "# If it loaded correctly, you can now update your extraction function to use it:\n",
    "if loaded_model:\n",
    "    # Update the global variables so the rest of your code uses the loaded version\n",
    "    model = loaded_model\n",
    "    processor = loaded_processor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36af3ada",
   "metadata": {},
   "source": [
    "### For Large Training of Entire 5,000 - 20,000 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dd9189",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"try:\n",
    "    # Initialize Model\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "    model.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids(['<s>'])[0]\n",
    "    model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "    model.decoder.resize_token_embeddings(len(processor.tokenizer))\n",
    "    model.config.encoder.image_size = image_size[::-1]\n",
    "    model.config.decoder.max_length = max_length\n",
    "    \n",
    "    # --- REAL TRAINING CONFIGURATION ---\n",
    "    # Calculate steps based on dataset size\n",
    "    # If you have 10,000 images and batch size 2, one epoch is 5,000 steps.\n",
    "    # A good training run is usually 3 to 5 epochs.\n",
    "    \n",
    "    # Assuming roughly 5,000 - 10,000 images in IDNet:\n",
    "    REAL_STEPS = 5000  \n",
    "    \n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"training_output\",\n",
    "        \n",
    "        # 1. INCREASE STEPS\n",
    "        max_steps=REAL_STEPS,           # Changed from 100 to 5000\n",
    "        \n",
    "        # 2. OPTIMIZE LEARNING RATE\n",
    "        learning_rate=1e-5,             # Slightly lower rate for stability over long runs\n",
    "        warmup_steps=200,               # Gently ramp up speed at the start\n",
    "        \n",
    "        # 3. BATCH SIZE (Crucial for GPU Memory)\n",
    "        # If you get \"CUDA Out of Memory\", change this to 1\n",
    "        per_device_train_batch_size=2,  \n",
    "        gradient_accumulation_steps=4,  # Simulates a batch size of 8 (2 * 4) for better convergence\n",
    "        \n",
    "        # 4. HARDWARE ACCELERATION\n",
    "        fp16=torch.cuda.is_available(), # Use mixed precision (faster, less memory)\n",
    "        \n",
    "        # 5. LOGGING & SAVING\n",
    "        logging_steps=100,              # Log progress every 100 steps\n",
    "        save_steps=1000,                # Save a checkpoint every 1000 steps (so you don't lose progress)\n",
    "        save_total_limit=2,             # Only keep the last 2 checkpoints to save disk space\n",
    "        remove_unused_columns=True,\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model, \n",
    "        args=training_args, \n",
    "        train_dataset=shuffled_dataset, \n",
    "    )\n",
    "\n",
    "    print(f\" Starting REAL Training for {REAL_STEPS} steps...\")\n",
    "    print(\"This may take several hours. Monitor the loss (it should go down).\")\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Save final model\n",
    "    trainer.save_model(model_save_path)\n",
    "    processor.save_pretrained(model_save_path)\n",
    "    print(f\" Training Complete! Model saved to {model_save_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Training failed: {e}\")\n",
    "    # Troubleshooting tip\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        print(\" TIP: Reduce 'per_device_train_batch_size' to 1 and restart.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca40bb1",
   "metadata": {},
   "source": [
    "### Continued Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c6a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# INDEXING (Uses all Folders)\n",
    "# ==========================================\n",
    "print(\"  Building Indexes from loaded dataset...\")\n",
    "\n",
    "face_embeddings = []\n",
    "text_documents = []\n",
    "metadata_store = []\n",
    "\n",
    "for idx, item in enumerate(tqdm(hf_dataset)):\n",
    "    image = item['image'].convert(\"RGB\")\n",
    "    text_data = item['text'] # Already a string\n",
    "    \n",
    "    # 1. Face Embedding\n",
    "    open_cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "    faces = face_app.get(open_cv_image)\n",
    "    \n",
    "    if len(faces) > 0:\n",
    "        emb = faces[0].normed_embedding.astype('float32')\n",
    "    else:\n",
    "        emb = np.zeros(512).astype('float32')\n",
    "        \n",
    "    face_embeddings.append(emb)\n",
    "    text_documents.append(text_data)\n",
    "    \n",
    "    metadata_store.append({\n",
    "        \"id\": idx,\n",
    "        \"data\": text_data\n",
    "    })\n",
    "\n",
    "# FAISS Setup\n",
    "face_embeddings = np.array(face_embeddings).astype('float32')\n",
    "d = 512\n",
    "if torch.cuda.is_available():\n",
    "    res = faiss.StandardGpuResources()\n",
    "    face_index = faiss.GpuIndexFlatL2(res, d)\n",
    "else:\n",
    "    face_index = faiss.IndexFlatL2(d)\n",
    "face_index.add(face_embeddings)\n",
    "\n",
    "# TF-IDF Setup\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "text_matrix = tfidf_vectorizer.fit_transform(text_documents)\n",
    "\n",
    "print(\" Search Indexes Ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fe4507",
   "metadata": {},
   "source": [
    "## Save and Load Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b69094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 1. Save Metadata Store\n",
    "with open(\"metadata_store.pkl\", \"wb\") as f:\n",
    "    pickle.dump(metadata_store, f)\n",
    "\n",
    "# 2. Save FAISS Index\n",
    "# Note: GPU indexes must be moved to CPU before saving\n",
    "if torch.cuda.is_available():\n",
    "    cpu_index = faiss.index_gpu_to_cpu(face_index)\n",
    "    faiss.write_index(cpu_index, \"face_index.bin\")\n",
    "else:\n",
    "    faiss.write_index(face_index, \"face_index.bin\")\n",
    "\n",
    "# 3. Save TF-IDF Vectorizer and Matrix\n",
    "with open(\"tfidf_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump((tfidf_vectorizer, text_matrix), f)\n",
    "\n",
    "print(\" All indexes and metadata saved to disk!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ad4dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Check if files exist first\n",
    "if os.path.exists(\"metadata_store.pkl\") and os.path.exists(\"face_index.bin\"):\n",
    "    print(\"‚è≥ Loading indexes from disk...\")\n",
    "\n",
    "    # 1. Load Metadata\n",
    "    with open(\"metadata_store.pkl\", \"rb\") as f:\n",
    "        metadata_store = pickle.load(f)\n",
    "\n",
    "    # 2. Load FAISS Index\n",
    "    face_index = faiss.read_index(\"face_index.bin\")\n",
    "    # Optional: Move back to GPU for speed\n",
    "    if torch.cuda.is_available():\n",
    "        res = faiss.StandardGpuResources()\n",
    "        face_index = faiss.index_cpu_to_gpu(res, 0, face_index)\n",
    "\n",
    "    # 3. Load TF-IDF\n",
    "    with open(\"tfidf_data.pkl\", \"rb\") as f:\n",
    "        tfidf_vectorizer, text_matrix = pickle.load(f)\n",
    "\n",
    "    print(f\" Loaded {len(metadata_store)} items from disk. Ready to search!\")\n",
    "else:\n",
    "    print(\" No saved indexes found. Please run the 'Indexing' block first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5902161d",
   "metadata": {},
   "source": [
    "## Continue Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98de4a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_donut(image):\n",
    "    \"\"\"\n",
    "    Extracts text using the GLOBALLY loaded model and processor.\n",
    "    This is much faster because we don't reload the model for every single image.\n",
    "    \"\"\"\n",
    "    # Safety Check: Ensure model is actually loaded\n",
    "    if 'model' not in globals() or 'processor' not in globals() or model is None:\n",
    "        print(\"‚ö†Ô∏è Model not loaded! Running fallback text extraction...\")\n",
    "        return \"\" \n",
    "\n",
    "    device = model.device # Get device from the model itself (cpu or cuda)\n",
    "\n",
    "    # 1. Prepare Image\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    \n",
    "    # 2. Prepare Prompt (Start Token)\n",
    "    task_prompt = \"<s>\"\n",
    "    decoder_input_ids = processor.tokenizer(\n",
    "        task_prompt, \n",
    "        add_special_tokens=False, \n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids.to(device)\n",
    "    \n",
    "    # 3. Generate Output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            pixel_values, \n",
    "            decoder_input_ids=decoder_input_ids, \n",
    "            max_length=512,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "    \n",
    "    # 4. Decode to String\n",
    "    sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "    \n",
    "    # 5. Clean up the output string (Remove <s> and </s> tokens)\n",
    "    sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "    # Also remove the start token if it appears\n",
    "    sequence = sequence.replace(\"<s>\", \"\")\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "def search_pipeline(query_image_path, k=5, alpha=0.5):\n",
    "    \"\"\"Hybrid search using Face Embeddings and OCR Text.\"\"\"\n",
    "    query_img = Image.open(query_image_path).convert(\"RGB\")\n",
    "    img_cv2 = cv2.imread(query_image_path)\n",
    "    \n",
    "    # Face Scores\n",
    "    face_scores = np.zeros(len(metadata_store))\n",
    "    if alpha > 0:\n",
    "        faces = face_app.get(img_cv2)\n",
    "        if len(faces) > 0:\n",
    "            q_emb = faces[0].normed_embedding.reshape(1, -1).astype('float32')\n",
    "            D, I = face_index.search(q_emb, k * 10) # Search wider then filter\n",
    "            for rank, idx in enumerate(I[0]):\n",
    "                if idx == -1: continue\n",
    "                face_scores[idx] = 1 / (1 + D[0][rank])\n",
    "\n",
    "    # Text Scores\n",
    "    text_scores = np.zeros(len(metadata_store))\n",
    "    if alpha < 1:\n",
    "        extracted_text = extract_text_donut(query_img)\n",
    "        q_vec = tfidf_vectorizer.transform([extracted_text])\n",
    "        sims = cosine_similarity(q_vec, text_matrix).flatten()\n",
    "        text_scores = sims\n",
    "\n",
    "    # Combine Scores\n",
    "    final_scores = (alpha * face_scores) + ((1 - alpha) * text_scores)\n",
    "    top_indices = np.argsort(final_scores)[::-1][:k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            \"id\": idx,\n",
    "            \"score\": final_scores[idx],\n",
    "            \"metadata\": metadata_store[idx]\n",
    "        })\n",
    "    return results, query_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088002d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "print(\" Select a query ID card image...\")\n",
    "query_path = select_file(\"Select Query Image\")\n",
    "\n",
    "if query_path:\n",
    "    print(f\"üîé Searching for: {query_path}\")\n",
    "    \n",
    "    # User Input for Search Mode\n",
    "    mode_map = {'1': \"Face Only\", '2': \"Text Only\", '3': \"Hybrid\"}\n",
    "    mode_input = input(\"Select Mode: [1] Face Only  [2] Text Only  [3] Hybrid (Default): \")\n",
    "    alpha = 1.0 if mode_input == '1' else (0.0 if mode_input == '2' else 0.5)\n",
    "    selected_mode = mode_map.get(mode_input, \"Hybrid\")\n",
    "    \n",
    "    # Search Pipeline (k=10 for detailed audit)\n",
    "    K_RESULTS = 10\n",
    "    results, q_img = search_pipeline(query_path, k=K_RESULTS, alpha=alpha)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\" AUDIT REPORT | Mode: {selected_mode} | Alpha: {alpha}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # --- VISUALIZATION SETUP ---\n",
    "    # Calculate grid size dynamically (e.g. 2 rows of 6)\n",
    "    total_plots = len(results) + 1\n",
    "    cols = 6\n",
    "    rows = math.ceil(total_plots / cols)\n",
    "    \n",
    "    plt.figure(figsize=(20, 4 * rows))\n",
    "    \n",
    "    # Plot 1: The Query Image\n",
    "    plt.subplot(rows, cols, 1)\n",
    "    plt.imshow(q_img)\n",
    "    plt.title(\"QUERY INPUT\\n(Source Artifact)\", color='blue', weight='bold')\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # --- DETAILED RESULT LOOP ---\n",
    "    for i, res in enumerate(results):\n",
    "        rank = i + 1\n",
    "        score = res['score']\n",
    "        \n",
    "        # 1. Deep Metadata Extraction\n",
    "        # The 'data' field is a JSON string we need to parse\n",
    "        try:\n",
    "            meta_dict = json.loads(res['metadata']['data'])\n",
    "            is_fraud = meta_dict.get('is_fraud', False)\n",
    "            fraud_type = meta_dict.get('fraud_type', 'None')\n",
    "            \n",
    "            # Extract PII for Audit (Obfuscated for privacy if needed)\n",
    "            audit_id = meta_dict.get('id', 'N/A')\n",
    "            audit_name = meta_dict.get('name', 'N/A')\n",
    "            audit_dob = meta_dict.get('dob', 'N/A')\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            is_fraud = False\n",
    "            fraud_type = \"METADATA_ERROR\"\n",
    "            audit_id = \"ERR\"\n",
    "            \n",
    "        # 2. Status Logic\n",
    "        status_label = \"üö® FRAUD DETECTED\" if is_fraud else \"‚úÖ AUTHENTIC\"\n",
    "        status_color = 'red' if is_fraud else 'green'\n",
    "        \n",
    "        # 3. Console Audit Log (Extreme Detail)\n",
    "        print(f\"\\n[RANK #{rank}] Score: {score:.4f} | Status: {status_label}\")\n",
    "        print(f\"   ‚îú‚îÄ  Database ID: {res['id']}\")\n",
    "        print(f\"   ‚îú‚îÄ  Fraud Type:  {fraud_type.upper()}\")\n",
    "        print(f\"   ‚îú‚îÄ  Document Metadata:\")\n",
    "        print(f\"   ‚îÇ    ‚îú‚îÄ Name: {audit_name}\")\n",
    "        print(f\"   ‚îÇ    ‚îú‚îÄ DOB:  {audit_dob}\")\n",
    "        print(f\"   ‚îÇ    ‚îî‚îÄ File: {meta_dict.get('file_name', 'Unknown')}\")\n",
    "        print(f\"   ‚îî‚îÄ  Match Logic: (Alpha={alpha})\")\n",
    "        \n",
    "        # 4. Visualization Plot\n",
    "        match_img = hf_dataset[int(res['id'])]['image']\n",
    "        \n",
    "        plt.subplot(rows, cols, i + 2)\n",
    "        plt.imshow(match_img)\n",
    "        \n",
    "        # Detailed Plot Title\n",
    "        title_text = (\n",
    "            f\"#{rank} {status_label}\\n\"\n",
    "            f\"Score: {score:.2f}\\n\"\n",
    "            f\"Type: {fraud_type}\"\n",
    "        )\n",
    "        plt.title(title_text, color=status_color, fontsize=9)\n",
    "        \n",
    "        # Add a border color to the image based on status\n",
    "        # (Matplotlib hack: draw a box around the axis)\n",
    "        ax = plt.gca()\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_edgecolor(status_color)\n",
    "            spine.set_linewidth(3)\n",
    "            \n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" End of Audit Report\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "else:\n",
    "    print(\" Selection Cancelled: No file selected.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace",
   "language": "python",
   "name": "workspace"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
